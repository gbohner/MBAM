\documentclass[10pt]{amsart}

\usepackage{amssymb}
%\input{macros}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-2in}
\calclayout


\newcommand\Ld{\sqrt[1/4]{L_0}D}

\newcommand\Lc{\sqrt[1/4]{L_0}C}


\newcommand\Le{\frac{E}{\sqrt[1/4]{L_0}}}

\newcommand\Po{P_o}

\newcommand\po{P_o}

\newcommand\lpo{\log(\Po)}

\newcommand\ca{\rm{Ca}^{2+}}

\newcommand\kk{\rm{K}^+}

\newcommand\kd{K_D}

\newcommand\pio{p(\mathcal{I},\mathcal{O})}

\newcommand{\ltwo}{\log_2}

\newcommand{\no}{N_{open}}
\newcommand{\ono}{\overline{\no}}
\newcommand{\var}{\sigma^2_{\rm N_{open}}}
\newcommand{\lo}{L_o}
\newcommand{\jo}{J_o}
\newcommand{\zl}{z_L}
\newcommand{\zj}{z_J}
%\\renewcommand{\b}[1]{\left( #1 \right)

\begin{document}

\title{Identifiability, reducibility, and evolvability in allosteric macromolecules}

%\author{Gerg\H{o} Bohner, \ Laurence Aitchison\and Gaurav Venkataraman}

\date{\vspace{-.1in}}

\maketitle


\section{Methods}


\subsection{Manifold Boundary Approximation Method}
MBAM attempts to find a simplified parameterisation of a model given an assay. In our context the model is the canonical MWC model of the BK ion channel described in \eqref{aldrich}. The assay consists of two concepts: (1) which voltages and calcium concentrations do we take measurements at and (2) how do we evaluate the model output - in our case either looking at the open probabilities or the base 10 logarithms thereof.

Let our model be a function $f()$. As input it takes an $M\times 1$ vector of parameters $\theta$ and an $N\times D$ array of measurements locations $\mathbf{X}$, with $N$ being the number of measurements and $D$ being the types of measurements, 2 for our assays.
The model's output is an $Nx1$ vector of measurements $\mathbf{y}$.

Given a measured data vector $\mathbf{y^*}$, we can find the best fit parameters $\theta^*$ by various optimization methods. In this paper we ask the question whether or not you can trust those optimzied paremeters, and the answer is often no. However, we do not stop at this disappointing finding, but go a step further, and attempt to find a set of parameters that can be reliably estimated.

In each iteration MBAM reduces the number of parameters by one, resulting in an $(M-1) \times 1$ vector $\Phi$, while still fitting the data well: $\|f(\Phi, \mathbf{X}) - \mathbf{y^*}\|$ is small. Even in the absence of data, the properties of models can be investigated by minimising the discrepency between the full and the reduced model: $\mathcal{C} = \|f(\Phi, \mathbf{X}) - f(\theta^*, \mathbf{X})\|$.



Armed with a \textit{goal} of reducing the number of parameters as well as a \textit{cost} for increasing discrepency, we can now write down the MBAM algorithm as follows: 
(1) Let $\mathbf{r} = f(\theta_t, \mathbf{X}) - f(\theta^*, \mathbf{X})$ be the vector of residuals given $\theta_t$. 
(2) Let $\mathbf{H}_t = \frac{d^2}{d\theta_t^2}\mathcal{C}$ be the Hessian of the cost function evaluated at the parameters $\theta_t$.
(3) MBAM attempts to find regions of the parameters space where one or more of the parameters are diverging in a coordinated way, while the cost is not. This is achieved by following a geodesic - a curve that locally increases the cost minimally - described by this ODE system:
	\begin{align}
		\frac{d}{d\tau}\theta_t&=\mathbf{v_t} \\
		\frac{d^2}{d\tau^2}\theta_t&=[(\nabla \mathbf{r}) (\nabla \mathbf{r})^\top]^{-1} (\nabla \mathbf{r}) \frac{\mathbf{v_t}^\top \mathbf{H}_t \mathbf{v_t}}{\|\mathbf{v_t}\|^2}
	\end{align}

	with the initial point given by $\theta_0 = \theta^*$ and the initial direction $\mathbf{v_0}$ is the eigenvector belonging to the smallest eigenvalue of $\mathbf{H}_0$. That is, we start from the best fit point - the lowest point on our cost surface, and always choose the direction that goes uphill as little as possible.

	We then follow this geodesic until (1) either the cost reaches a threshold and we claim that the set of parameters cannot be reduced further without sacrificing the quality of it; (2) or some of the parameters diverge in a coordinated way. This can be monitored by following the evolution of the eigenvalues and eigenvectors of the local metric $[(\nabla \mathbf{r}) (\nabla \mathbf{r})^\top]$. In the latter case we can write down a reduced model $\Phi$ by appropriate elimination or combination of the diverging parameters and proceed with the next iteration of MBAM.

	\subsection{Estimating parameter uncertainties}
	\subsubsection{Theoretical lower bound}
	A well-established method of estimating the confidence in our fitted parameter values is to rely on the quadratic approximation to our cost function around the best fit parameters. This is done by computing the Hessian $\mathbf{H}_0$ around $\theta^*$, that is already used by MBAM. Then the relative size of the 95\% confidence interval for the $i$-th parameter can be estimated as:
	\begin{align}
		\Sigma_i = \exp\left(4\times\left(\frac{\sigma^2}{N}\left(\mathbf{H}_0^{-1}\right)_{i,i}\right)^{1/2}\right) - 1,
	\end{align}

	where $N$ is the number of datapoints and $\sigma$ is the relative measurement error. For all our calculations, $N=104$ and $\sigma=10\%$.

	\subsubsection{Fitting simulated data under noise}
	The above bound is (1) a \textit{lower bound} to the actual parameter uncertainty and (2) is only valid for \textit{small} values of $\sigma$, where small is unfortunately very loosely defined and highly model- and assay-dependent. A more reliable, but computationally much more expensive way of estimating our error is by actually carrying out the procedure that the theoretical bound assumes; that is, to simulate noisy measurement from the now true parameters $\theta^*$ then using an optimization algorithm, find a new best fit $\theta^{'}$ to that noisy data. Repeating this multiple times will give us a set of new best fits; based on this assembly we can then estimate our individual relative parameter uncertainties by taking the standard deviation of the parameter of interest. Formally:
	\begin{enumerate}
		\item Simulate noisy data as $\mathbf{y'} = f(\theta^*, \mathbf{X})\odot\mathbf{\epsilon}(\sigma)$, where $\epsilon \sim \textrm{Uniform}([1-\sigma, 1+\sigma])$ is a realisation of the noise parameterised by $\sigma$.
		\item Find $\theta^{'} = \textrm{argmin}_\theta \left[ \|f(\theta, \mathbf{X}) - \mathbf{y'}\|\right]$. For the optimization we used a Levenberg-Marquardt solver initialized from 24 different parameter vectors, and the best fit was chosen as the lowest cost after all have converged to local minima.
		\item Repeat the above procedure $J$ times for each different choice of true parameters, model or assay.
		\item Then the individual parameter uncertainties are $$\Sigma_i =  \exp\left(4\times \textrm{std}\left(\left\{\log\left(\theta^{'}_j\right)_i\right\}_{j=1}^J\right) \right) - 1$$. 
	\end{enumerate}



\end{document}